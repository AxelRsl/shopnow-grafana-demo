// Grafana Alloy Configuration for ShopNow Demo    
// This configuration collects metrics, logs, and traces and sends them to Grafana Cloud    

// ====================================    
// LOGGING CONFIGURATION    
// ====================================    
logging {    
  level  = "info"    
  format = "logfmt"    
}    

// ====================================    
// OTLP RECEIVER    
// Receives telemetry from OpenTelemetry instrumented services    
// ====================================    
otelcol.receiver.otlp "default" {    
  // gRPC endpoint    
  grpc {    
    endpoint = "0.0.0.0:4317"    
  }    
  
  // HTTP endpoint    
  http {    
    endpoint = "0.0.0.0:4318"    
  }    

  output {    
    metrics = [otelcol.processor.batch.default.input]    
    logs    = [otelcol.processor.batch.default.input]    
    traces  = [otelcol.processor.batch.default.input]    
  }    
}    

// ====================================    
// BATCH PROCESSOR    
// Batches telemetry data for efficient transmission    
// ====================================    
otelcol.processor.batch "default" {    
  timeout          = "5s"    
  send_batch_size  = 1024    
  
  output {    
    metrics = [otelcol.processor.attributes.add_labels.input]    
    logs    = [otelcol.processor.attributes.add_labels.input]    
    traces  = [otelcol.processor.attributes.add_labels.input]    
  }    
}    

// ====================================    
// ATTRIBUTES PROCESSOR    
// Adds common labels to all telemetry    
// ====================================    
otelcol.processor.attributes "add_labels" {    
  action {    
    key    = "environment"    
    value  = "demo"    
    action = "upsert"    
  }    
  
  action {    
    key    = "application"    
    value  = "shopnow"    
    action = "upsert"    
  }    
  
  output {    
    metrics = [otelcol.exporter.prometheus.grafana_cloud.input]    
    logs    = [otelcol.exporter.loki.grafana_cloud.input]    
    traces  = [otelcol.exporter.otlp.grafana_cloud.input]    
  }    
}    

// ====================================    
// PROMETHEUS EXPORTER    
// Converts OTLP metrics to Prometheus format    
// ====================================    
otelcol.exporter.prometheus "grafana_cloud" {    
  forward_to = [prometheus.remote_write.grafana_cloud.receiver]    
}    

// ====================================      
// PROMETHEUS REMOTE WRITE      
// Sends metrics to Grafana Cloud Mimir      
// ====================================      
prometheus.remote_write "grafana_cloud" {      
  endpoint {      
    url = sys.env("GRAFANA_CLOUD_PROMETHEUS_URL")      
    
    basic_auth {      
      username = sys.env("GRAFANA_CLOUD_PROMETHEUS_USER")      
      password = sys.env("GRAFANA_CLOUD_PROMETHEUS_PASSWORD")  // ⬅️ Cambia de GRAFANA_CLOUD_API_KEY  
    }      
    
    queue_config {      
      capacity             = 10000      
      max_shards           = 10       // Reducido de 50 a 10 para evitar rate limit      
      max_samples_per_send = 1000     // Reducido de 5000 a 1000      
      batch_send_deadline  = "10s"    // Espera 10s antes de enviar lotes pequeños      
    }      
  }      
}      

// ====================================      
// LOKI EXPORTER      
// Sends logs to Grafana Cloud Loki      
// ====================================      
otelcol.exporter.loki "grafana_cloud" {      
  forward_to = [loki.relabel.otlp_logs.receiver]      
}

// Pipeline for OTLP logs (already structured, no parsing needed)
loki.relabel "otlp_logs" {
  forward_to = [loki.write.grafana_cloud.receiver]
  
  // Copy service_name to service for Drilldown compatibility
  rule {
    source_labels = ["service_name"]
    target_label  = "service"
  }
}

loki.write "grafana_cloud" {      
  endpoint {      
    url = sys.env("GRAFANA_CLOUD_LOKI_URL")      
    
    basic_auth {      
      username = sys.env("GRAFANA_CLOUD_LOKI_USER")      
      password = sys.env("GRAFANA_CLOUD_LOKI_PASSWORD")
    }
  }
}  

// ====================================      
// TEMPO EXPORTER      
// Sends traces to Grafana Cloud Tempo via OTLP/gRPC      
// ====================================      
otelcol.exporter.otlp "grafana_cloud" {      
  client {      
    endpoint = sys.env("GRAFANA_CLOUD_TEMPO_ENDPOINT")  
    auth     = otelcol.auth.basic.grafana_cloud_tempo.handler      
  }      
}      

otelcol.auth.basic "grafana_cloud_tempo" {  
  username = sys.env("GRAFANA_CLOUD_TEMPO_USER")  
  password = sys.env("GRAFANA_CLOUD_TEMPO_PASSWORD")  
}   

// ====================================
// PYROSCOPE PROFILING
// Receives and sends continuous profiling data
// ====================================
pyroscope.receive_http "default" {
  http {
    listen_address = "0.0.0.0"
    listen_port    = 4040
  }

  forward_to = [pyroscope.write.grafana_cloud.receiver]
}

pyroscope.write "grafana_cloud" {
  endpoint {
    url = sys.env("GRAFANA_CLOUD_PYROSCOPE_URL")
    
    basic_auth {
      username = sys.env("GRAFANA_CLOUD_PYROSCOPE_USER")
      password = sys.env("GRAFANA_CLOUD_PYROSCOPE_PASSWORD")
    }
  }
}

// ====================================    
// SELF-MONITORING    
// Alloy monitors itself    
// ====================================    
prometheus.scrape "alloy" {    
  targets = [{    
    __address__ = "localhost:12345",    
    job         = "alloy",    
  }]    
  
  forward_to = [prometheus.remote_write.grafana_cloud.receiver]    
}    

// ====================================    
// DOCKER CONTAINER DISCOVERY
// Discovers all Docker containers
// ====================================    
discovery.docker "containers" {    
  host = "unix:///var/run/docker.sock"    
}

// Relabel to only scrape containers with Prometheus metrics
discovery.relabel "prometheus_metrics" {
  targets = discovery.docker.containers.targets
  
  // Only scrape containers with metrics port exposed
  // This avoids trying to scrape databases which triggers security warnings
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = "/(shopnow-alloy|shopnow-api-gateway|shopnow-order-service)"
    action        = "keep"
  }
}

prometheus.scrape "containers" {    
  targets    = discovery.relabel.prometheus_metrics.output    
  forward_to = [prometheus.remote_write.grafana_cloud.receiver]    
  
  scrape_interval = "15s"    
  scrape_timeout  = "10s"    
}    

// ====================================  
// DOCKER LOGS - MÉTODO ROBUSTO VIA ARCHIVOS  
// Usa loki.source.file en lugar de loki.source.docker  
// ====================================  

// Relabeling para agregar metadata de contenedores  
discovery.relabel "docker_logs" {  
  targets = discovery.docker.containers.targets  

  // Mantener solo contenedores shopnow-*  
  rule {  
    source_labels = ["__meta_docker_container_name"]  
    regex         = "/shopnow-.*"  
    action        = "keep"  
  }  

  // Construir path al archivo de log  
  rule {  
    source_labels = ["__meta_docker_container_id"]  
    target_label  = "__path__"  
    replacement   = "/var/lib/docker/containers/${1}/${1}-json.log"  
  }  

  // Extraer nombre del contenedor  
  rule {  
    source_labels = ["__meta_docker_container_name"]  
    regex         = "/?(.*)"  
    target_label  = "container"  
  }  

  // service_name desde label  
  rule {  
    source_labels = ["__meta_docker_container_label_com_grafana_service"]  
    regex         = "(.+)"  
    target_label  = "service_name"  
  }  

  // Fallback service_name desde nombre  
  rule {  
    source_labels = ["__meta_docker_container_name"]  
    regex         = "/shopnow-(.*)"  
    target_label  = "service_name"  
  }  

  // job label  
  rule {  
    source_labels = ["__meta_docker_container_label_com_grafana_job"]  
    regex         = "(.+)"  
    target_label  = "job"  
  }  

  // project label  
  rule {  
    source_labels = ["__meta_docker_container_label_com_grafana_project"]  
    regex         = "(.+)"  
    target_label  = "project"  
  }  

  // container_id  
  rule {  
    source_labels = ["__meta_docker_container_id"]  
    target_label  = "container_id"  
  }  

  // service para Drilldown  
  rule {  
    source_labels = ["service_name"]  
    target_label  = "service"  
  }  
}  

// Leer logs de archivos  
loki.source.file "docker_logs" {  
  targets    = discovery.relabel.docker_logs.output  
  forward_to = [loki.process.docker_json.receiver]  
}  

// Procesar formato JSON de Docker  
loki.process "docker_json" {  
  forward_to = [loki.process.parse_logs.receiver]  

  // Docker logs vienen en formato JSON  
  stage.json {  
    expressions = {  
      output = "log",  
      stream = "stream",  
      time   = "time",  
    }  
  }  

  // Usar el contenido del log  
  stage.output {  
    source = "output"  
  }  

  // Timestamp del log  
  stage.timestamp {  
    source = "time"  
    format = "RFC3339Nano"  
  }  
}

// ====================================    
// LOG PROCESSING PIPELINE    
// Parses and transforms logs before sending to Loki    
// ====================================    
loki.process "parse_logs" {    
  forward_to = [loki.write.grafana_cloud.receiver]    
  
  // ========================================    
  // MONGODB: Parse JSON and map s=I/W/E/F    
  // ========================================    
  stage.match {    
    selector = "{service_name=\"mongodb\"}"    
    
    stage.json {    
      expressions = {    
        mongo_severity  = "s",    
        mongo_component = "c",    
        mongo_message   = "msg",    
        mongo_ctx       = "ctx",    
      }    
    }    
    
    stage.template {    
      source   = "detected_level"    
      template = "{{ if eq .mongo_severity \"I\" }}info{{ else if eq .mongo_severity \"W\" }}warn{{ else if eq .mongo_severity \"E\" }}error{{ else if eq .mongo_severity \"F\" }}critical{{ else if eq .mongo_severity \"D\" }}debug{{ else }}info{{ end }}"    
    }    
    
    stage.labels {    
      values = {    
        detected_level = "",    
      }    
    }    
  }    
  
  // ========================================    
  // POSTGRESQL: Extract level from text    
  // ========================================    
  stage.match {    
    selector = "{service_name=\"postgres\"}"    
    
    stage.regex {    
      expression = "(?P<pg_level>LOG|ERROR|WARNING|FATAL|PANIC|INFO|DEBUG)"    
    }    
    
    stage.template {    
      source   = "detected_level"    
      template = "{{ if eq .pg_level \"LOG\" }}info{{ else if eq .pg_level \"INFO\" }}info{{ else if eq .pg_level \"DEBUG\" }}debug{{ else if eq .pg_level \"WARNING\" }}warn{{ else if eq .pg_level \"ERROR\" }}error{{ else if eq .pg_level \"FATAL\" }}critical{{ else if eq .pg_level \"PANIC\" }}critical{{ else }}info{{ end }}"    
    }    
    
    stage.labels {    
      values = {    
        detected_level = "",    
      }    
    }    
  }    
  
  // ========================================    
  // REDIS: Parse Redis log format    
  // ========================================    
  stage.match {    
    selector = "{service_name=\"redis\"}"    
    
    // Extraer símbolo Redis    
    stage.regex {    
      expression = "\\[\\d+\\]\\s+\\d+\\s+\\w+\\s+[\\d:]+\\.\\d+\\s+(?P<redis_symbol>[*#.-])"    
    }    
    
    // Detectar si hay SECURITY en el mensaje    
    stage.regex {    
      expression = "(?i)(?P<has_security>SECURITY)"    
    }    
    
    // Determinar nivel en UN SOLO template    
    stage.template {    
      source   = "detected_level"    
      template = "{{ if .has_security }}error{{ else if eq .redis_symbol \".\" }}debug{{ else if eq .redis_symbol \"*\" }}info{{ else if eq .redis_symbol \"#\" }}warn{{ else if eq .redis_symbol \"-\" }}notice{{ else }}info{{ end }}"    
    }    
    
    stage.labels {    
      values = {    
        detected_level = "",    
      }    
    }    
  }    
  
  // ========================================    
  // NODE.JS SERVICES & ALLOY: Parse LOGFMT    
  // ========================================    
  stage.match {    
    selector = "{service_name=~\"api-gateway|order-service|payment-service|inventory-service|recommendation-service|alloy\"}"    
    
    // Parsear LOGFMT (level=info msg="..." etc.)    
    stage.logfmt {    
      mapping = {    
        extracted_level = "level",    
        msg             = "msg",    
        component_id    = "component_id",    
      }    
    }    
    
    // Mapear el nivel extraído    
    stage.template {    
      source   = "detected_level"    
      template = "{{ if .extracted_level }}{{ .extracted_level | ToLower }}{{ else }}info{{ end }}"    
    }    
    
    stage.labels {    
      values = {    
        detected_level = "",    
      }    
    }    
  }    
  
  // ========================================    
  // FALLBACK: Si no se detectó nivel, asignar info    
  // ========================================    
  stage.match {    
    selector = "{detected_level=\"\"}"    
    
    stage.template {    
      source   = "detected_level"    
      template = "info"    
    }    
    
    stage.labels {    
      values = {    
        detected_level = "",    
      }    
    }    
  }    
}  
